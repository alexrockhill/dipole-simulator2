{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# About Dipoles in MEG and EEG\n\nFor an explanation of what is going on in the demo and background information\nabout magentoencephalography (MEG) and electroencephalography (EEG) in\ngeneral, let's walk through some code. To execute this code, you'll need\nto have a working version of python with ``mne`` installed, see the\n`quick-start` documentation for instructions. You'll need the development\nversion, so you'll need to do\n``pip install git+https://github.com/mne-tools/mne-python.git`` You'll also\nneed to install the requirements such as with\n``pip install -r requirements.txt``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Alex Rockhill <aprockhill@mailbox.org>\n#\n# License: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by importing the dependencies we'll need.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path as op  # comes with python and helps naviagte to files\nimport numpy as np  # a scientific computing package with arrays\nimport matplotlib.pyplot as plt  # a plotting library\nimport mne  # our main analysis software package\nfrom nilearn.plotting import plot_anat  # this package plots brains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background\nMEG and EEG researchers record very small electromagentic potentials\ngenerated by the brain from outside the head. When it comes from the\nrecording devices, it looks like this (there are a lot of channels\nso only a subset are shown):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path = mne.datasets.sample.data_path()  # get the sample data path\nraw = mne.io.read_raw(  # navigate to some raw sample data\n    op.join(data_path, 'MEG', 'sample', 'sample_audvis_raw.fif'))\nraw_plot = raw.copy()  # make a copy to modify for plotting\nraw_plot.pick_channels(raw.ch_names[::10])  # pick only every tenth channel\nraw_plot.plot(n_channels=len(raw_plot.ch_names),\n              duration=1,  # only a small, 1 second time window\n              start=50,  # start part way in\n              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of MEG and EEG researchers is to try and understand how activity\nin the brain changes as we respond to stimuli in our environment and\nperform behaviors. To do that, researchers will often use magnetic resonance\n(MR) to create an image of the research subject's brain. These images\nlook like this:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# first, get a T1-weighted MR scan file from the MNE example dataset\nT1_fname = op.join(data_path, 'subjects', 'sample', 'mri', 'T1.mgz')\nplot_anat(T1_fname)  # now we can plot it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The T1 MR image can be used to figure out where the surfaces of the\nbrain skull and scalp are as well as label the parts of the brain\nin the image using Freesurfer. The command below does this (it takes\n8 hours so I wouldn't recommend executing it now but it has already\nbeen done for you in the mne sample data, see `here\n<https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall>`_ for\nhow to install Freesurfer):\n\n.. code-block:: bash\n\n   recon-all -subjid sample -sd $DATA_PATH/subjects -i $T1_FNAME -all\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's put it all together and see the problem that MEG and EEG\nresearchers face in figuring out what's going on inside the brain from\nelectromagnetic potentials on the surface of the scalp. As you can see below,\nthere are a lot of MEG and EEG sensors and they cover a large portion of the\nhead but its not readily apparent how much of each brain area each sensor\nrecords from and how to separate the summed activity from all the brain areas\nthat is recorded by each sensor into components for each brain area:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The sensor positions don't come aligned to the MR image since they are\n    recorded by a different device so we need to a transformation matrix to\n    transform them from the coordinate frame they are in to MR coordinates.\n    This can be done with :func:`mne.gui.coregistration` to generate the\n    ``trans`` file that is loaded below.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the subjects_dir is where Freesurfer stored all the surface files\nsubjects_dir = op.join(data_path, 'subjects')\ntrans = mne.read_trans(op.join(data_path, 'MEG', 'sample',\n                               'sample_audvis_raw-trans.fif'))\n\n# the main plotter for mne, the brain object\nbrain = mne.viz.Brain(subject_id='sample', hemi='both', surf='pial',\n                      subjects_dir=subjects_dir)\nbrain.add_skull(alpha=0.5)  # alpha sets transparency\nbrain.add_head(alpha=0.5)\nbrain.add_sensors(raw.info, trans)\n# set a nice view to show\nbrain.show_view(azimuth=120, elevation=90, distance=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a Source Space and Forward Model\nFirst let's setup a space of vertices within the brain that we will consider\nas the sources of signal. In a real brain, there are hundreds of billions\nof cells but we don't have the resolution with only hundreds of sensors to\ndetermine the activity of each cell, so, instead, we'll choose a regularly\nsampled grid of sources that represent the summed activity of tens of\nthousands of cells. In most analyses in publications, the source space has\naround 8000 vertices, but, for this example, we'll use a smaller source\nspace for demonstration.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we would need to make a boundary element model (BEM) to account for\ndifferences in conductivity of the brain, skull and scalp. This can be\ndone with :func:`mne.make_bem_model` but, in this case, we'll just load\na pre-computed model. We'll also load the solution to the BEM model for how\ndifferent conductivities of issues effect current dipoles as they pass\nthrough each of the layers, but this can be computed with\n:func:`mne.make_bem_solution`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bem_fname = op.join(subjects_dir, 'sample', 'bem',\n                    'sample-5120-5120-5120-bem.fif')\n\n# load a pre-computed solution the how the sources within the brain will\n# be affected by the different conductivities\nbem_sol = op.join(subjects_dir, 'sample', 'bem',\n                  'sample-5120-5120-5120-bem-sol.fif')\n# plot it, it's saved out in a standard location,\n# so we don't have to pass the path\nmne.viz.plot_bem(subject='sample', subjects_dir=op.join(data_path, 'subjects'),\n                 slices=np.linspace(45, 200, 12).round().astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Making a Dipole\nNow, we're ready to make a dipole and see how its current will be recorded\nat the scalp with MEG and EEG.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can use ``print(mne.Dipole.__doc__)`` to print the arguments that\n    are required by ``mne.Dipole`` or any other class, method or function.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# make a dipole within the temporal lobe pointing superiorly,\n# fake a goodness-of-fit number\ndip_pos = [-0.0647572, 0.01315963, 0.07091921]\ndip = mne.Dipole(times=[0], pos=[dip_pos], amplitude=[3e-8],\n                 ori=[[0, 0, 1]], gof=50)\n\n# plot it!\nbrain = mne.viz.Brain(subject_id='sample', hemi='both', surf='pial',\n                      subjects_dir=subjects_dir, alpha=0.25)\nbrain.add_dipole(dip, trans, scales=10)\nbrain.show_view(azimuth=150, elevation=60, distance=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulating Sensor Data\nWe're ready to compute a forward operator using the BEM to make the so-called\nleadfield matrix which multiplies activity at the dipole to give the\nmodelled the activity at the sensors. We can then use this to simulate evoked\ndata.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fwd, stc = mne.make_forward_dipole(\n    dipole=dip, bem=bem_sol, info=raw.info, trans=trans)\n# we don't have a few things like the covarience matrix or a number of epochs\n# to average so we use these arguments for a reasonable solution\nevoked = mne.simulation.simulate_evoked(\n    fwd, stc, raw.info, cov=None, nave=np.inf)\n\n# Now we can see what it would look like at the sensors\nfig, axes = plt.subplots(1, 3, figsize=(6, 4))  # make a figure with 3 subplots\n# use zip to iterate over axes and channel types at the same time\nfor ax, ch_type in zip(axes, ('grad', 'mag', 'eeg')):\n    # we're just looking at the relative pattern so we won't use a colorbar\n    evoked.plot_topomap(times=[0], ch_type=ch_type, axes=ax, colorbar=False)\n    ax.set_title(ch_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrapping Up\nWe covered some good intuition but there's lots more to learn! The main thing\nis that MEG and EEG researchers generally don't have the information about\nwhat's going on inside the brain, that's what they are trying to predict. To\nreverse this process, you need to invert the forward solution (tutorial:\n`tut-viz-stcs`). There is tons more to explore in the MNE `tutorials\n<https://mne.tools/dev/auto_tutorials/index.html>`_ and `examples\n<https://mne.tools/dev/auto_examples/index.html>`_ pages. Let's leave off by\nsetting up a source space of many different dipoles and seeing their\ndifferent activities manifest on the scalp as measured by the sensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "src = mne.setup_volume_source_space(\n    subject='sample', pos=20,  # in mm\n    bem=bem_fname, subjects_dir=subjects_dir)\n\n# make the leadfield matrix\nfwd = mne.make_forward_solution(\n    raw.info, trans=trans, src=src, bem=bem_sol)\n\n# plot our setup\nbrain = mne.viz.Brain(subject_id='sample', hemi='both', surf='pial',\n                      subjects_dir=subjects_dir, alpha=0.25)\nbrain.add_volume_labels(alpha=0.25, colors='gray')\nbrain.add_forward(fwd, trans, scale=3)\nbrain.show_view(azimuth=30, elevation=90, distance=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the same solution using a source space of dipoles\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# take the source space from the forward model because some of the\n# vertices are excluded from the vertices in src\nn_dipoles = fwd['source_rr'].shape[0]  # rr is the vertex positions\n# find the closest dipole to the one we used before (it was in this\n# source space) using the euclidean distance (np.linalg.norm)\nidx = np.argmin(np.linalg.norm(fwd['source_rr'] - dip_pos, axis=1))\n# make an empty matrix of zeros\ndata = np.zeros((n_dipoles, 3, 1))\ndata[idx, 2, 0] = 3e-8  # make the same dipole as before\n# this is the format that vertiex numbers are stored in\nvertices = [fwd['src'][0]['vertno']]\nstc = mne.VolVectorSourceEstimate(data, vertices=vertices,\n                                  subject='sample', tmin=0, tstep=1)\n\nevoked = mne.simulation.simulate_evoked(\n    fwd, stc, raw.info, cov=None, nave=np.inf)\n\n# confirm our replication\nfig, axes = plt.subplots(1, 3, figsize=(6, 4))  # make a figure with 3 subplots\nfor ax, ch_type in zip(axes, ('grad', 'mag', 'eeg')):\n    evoked.plot_topomap(times=[0], ch_type=ch_type, axes=ax, colorbar=False)\n    ax.set_title(ch_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, go crazy and simulate a bunch of random dipoles\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(88)  # always seed random number generation for reproducibility\nstc.data = np.random.random(stc.data.shape) * 3e-8 - 1.5e-8\nevoked = mne.simulation.simulate_evoked(\n    fwd, stc, raw.info, cov=None, nave=np.inf)\n\n# now that's a complicated faked brain pattern, fortunately brain activity\n# is much more correlated (neighboring areas have similar activity) which\n# makes results a bit easier to interpret\nfig, axes = plt.subplots(1, 3, figsize=(6, 4))  # make a figure with 3 subplots\nfor ax, ch_type in zip(axes, ('grad', 'mag', 'eeg')):\n    evoked.plot_topomap(times=[0], ch_type=ch_type, axes=ax, colorbar=False)\n    ax.set_title(ch_type)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}